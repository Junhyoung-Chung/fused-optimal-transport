\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
% \usepackage{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
\usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 % \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
% \workshoptitle{WORKSHOP TITLE}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{mathtools}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{xcolor}
\usetikzlibrary{calc}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma} 
\newtheorem{proposition}{Proposition} 
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition} 
\newtheorem{example}{Example} 
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
\newtheorem{remark}{Remark}
\newtheorem{note}{Note} 
\newtheorem{question}{Question}

\crefname{theorem}{Theorem}{Theorems}
\Crefname{theorem}{Theorem}{Theorems}

\crefname{lemma}{Lemma}{Lemmas}
\Crefname{lemma}{Lemma}{Lemmas}

\crefname{proposition}{Proposition}{Propositions}
\Crefname{proposition}{Proposition}{Propositions}

\crefname{corollary}{Corollary}{Corollaries}
\Crefname{corollary}{Corollary}{Corollaries}

\crefname{definition}{Definition}{Definitions}
\Crefname{definition}{Definition}{Definitions}

\crefname{example}{Example}{Examples}
\Crefname{example}{Example}{Examples}

\crefname{assumption}{Assumption}{Assumptions}
\Crefname{assumption}{Assumption}{Assumptions}

\crefname{claim}{Claim}{Claims}
\Crefname{claim}{Claim}{Claims}

\crefname{remark}{Remark}{Remarks}
\Crefname{remark}{Remark}{Remarks}

\crefname{note}{Note}{Notes}
\Crefname{note}{Note}{Notes}

\crefname{question}{Question}{Questions}
\Crefname{question}{Question}{Questions}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote. 
\title{Learning Geometry Preserving Optimal Transport Plan via Convex Relaxation}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Junhyoung Chung\thanks{\href{https://junhyoung-chung.github.io/}{https://junhyoung-chung.github.io/}} \\
  Department of Statistics\\
  Seoul National University\\
  Seoul 08826, Republic of Korea \\
  \texttt{junhyoung0534@gmail.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}


\section{Introduction}

Optimal transport (OT) provides a powerful mathematical framework for comparing probability measures by quantifying the minimal cost of transporting mass from one distribution to another. In recent years, OT has found wide applications in statistics, machine learning, and computer vision, where distributions often lie on non-Euclidean or structured domains. However, in many real-world problems, each observation possesses both spatial and feature information---for example, geometric shapes with embedded descriptors, or spatially indexed random fields with associated features. In such settings, it is desirable to align not only the feature embeddings but also the underlying spatial structures.

To address this, we consider a \emph{fused optimal transport} (FOT) formulation, which simultaneously accounts for feature similarity and spatial coherence through a kernel-weighted coupling cost. This formulation generalizes both the classical quadratic OT and the Gromov--Wasserstein (GW) transport, providing a flexible interpolation between them. The rest of this section introduces the formal setup, notation, and basic existence results for the fused optimal transport plan.

\section{Methodology}

%%%% Notations %%%%
\paragraph{Notations.}
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $(S,d_S)$ be a compact metric space. 
A measurable map $X:\Omega\to S$ is called a random element with distribution $\mathbb{P}_X\coloneqq\mathbb{P}\circ X^{-1}$. Denote $L^2(S,\mathbb{P}_X)$ by the Hilbert space of real-valued, square integrable functions on $S$ with respect to $\mathbb{P}_X$.
We also introduce a feature space $M\subset\mathbb{R}^d$ which is compact, and call any one-to-one and continuous $f:S\to M$ a feature function. Throughout this study, we assume that $\mathrm{diam}(S) = \mathrm{diam}(M) = 1$, where $\mathrm{diam}(A) \coloneqq \sup_{x,x^\prime \in A}d_A(x,x^\prime)$. For two probability measures $\mu,\nu$ on $S$, denote by
\begin{align*}
	\Pi(\mu,\nu) \coloneqq \{\pi\ \text{on }S\times S:\ \text{the marginals are $\mu$ and $\nu$}\}
\end{align*}
the set of all couplings between $\mu$ and $\nu$. Given an arbitrary coupling $\pi \in \Pi(\mathbb{P}_X,\mathbb{P}_Y)$, the conditional expectation operator $T_{\pi}: L^2(S,\mathbb{P}_Y) \to L^2(S,\mathbb{P}_X)$ is defined as $(T_{\pi}g)(x) \coloneqq \mathbb{E}[g(Y) \mid X = x]$ for any $g \in L^2(S,\mathbb{P}_Y)$. Lastly, we say a measurable map $T: S \to S$ pushes forward $\mu$ to $\nu$ if $\mu(T^{-1}(A)) = \nu(A)$ for all $A \in \mathcal{B}(S)$, where $\mathcal{B}(S)$ is the Borel $\sigma$-algebra of $S$. We denote $T_{\#}\mu = \nu$ if $T$ pushes forward $\mu$ to $\nu$.
%By the disintegration theorem, for $\pi \in \Pi(\mu,\nu)$ there exists a Markov kernel $k(,\cdot,\cdot): \mathcal{B}(S) \times S \to [0,1]$ such that $\pi(dx,dy) = k(dy,x)\mu(dx)$, where $\mathcal{B}(S)$ is the Borel $\sigma$-algebra of $S$.

%%%% Fused GW %%%%
\paragraph{Fused Gromov-Wasserstein Discrepancy.}
For $0\le \alpha\le 1$ and a feature function $f$, \citet{vayer2020fused} propose the following optimization problem:
\begin{align}
	\label{eq:pi-fused-ot}
	\inf_{\pi\in\Pi(\mathbb{P}_X,\mathbb{P}_Y)} 
	(1-\alpha)&\mathbb{E}_{(X,Y)\sim\pi}\big[\|f(X)-f(Y)\|_2^2\big] \nonumber\\
	&+ \alpha\mathbb{E}_{\substack{(X,Y)\sim\pi\\(X^\prime,Y^\prime)\sim\pi}}
	\Big[\big|d_S(X,X^\prime)-d_S(Y,Y^\prime)\big|^2\Big] .
\end{align}
The first term enforces feature-wise alignment via $f$, while the second encourages structural consistency under the spatial metric $d_S$. When $\alpha = 0$, the problem reduces to classical quadratic OT; when $\alpha = 1$, it coincides with the Gromov–Wasserstein setting emphasizing relational geometry.
\begin{proposition}[Existence of a minimizer]
	\label{prop:existence}
	For each $0\leq \alpha \leq 1$, \eqref{eq:pi-fused-ot} admits at least one minimizer; that is, \eqref{eq:pi-fused-ot} is solvable.
\end{proposition}
\begin{proof}
	The proof can be found in \citet{vayer2020fused}.
\end{proof}
The existence follows from standard weak compactness of the set of couplings $\Pi(\mathbb{P}_X,\mathbb{P}_Y)$ and lower semicontinuity of the objective functional. However, the minimizer of \eqref{eq:pi-fused-ot} is not necessarily unique, and due to the non-convexity of the second (structural) term, the optimization landscape may contain multiple local minima. Consequently, standard numerical algorithms can only guarantee convergence to stationary or locally optimal solutions, rather than the global optimum. This highlights the importance of developing a convex reformulation or an appropriate convex relaxation of the fused Gromov–Wasserstein problem to ensure computational tractability and theoretical robustness.

%%% Proposed method %%%
\paragraph{Proposed method.} Our proposed method introduces a surrogate loss for the second term in \eqref{eq:pi-fused-ot}, thereby ensuring that the problem is convex.
\begin{definition}[Distance kernel]
	\label{def:kernel}
	For a fixed bandwidth $h > 0$, a function $K_h: S \times S \to \mathbb{R}_+$ is said to be a \emph{distance kernel} if there exists a bounded, (strictly) monotone $\kappa: \mathbb{R}_+ \to \mathbb{R}_+$ such that
	\begin{align*}
		K_h(x,x^\prime) = \frac{1}{h}\kappa\left(\frac{d_S(x,x^\prime)}{h}\right) .
	\end{align*}
\end{definition}
%We only require $\kappa$ to be strictly monotone, which means it may be either strictly decreasing or strictly increasing. 
%This assumption guarantees injectivity of the mapping $r \mapsto \kappa(r/h)$, so that equal kernel values correspond exactly to equal underlying distances; that is, $K_h(x,x') = K_h(T(x),T(x)')$ if and only if $d_S(x,x') = d_S(T(x),T(x'))$. 
The direction of monotonicity of $\kappa$ determines the geometric emphasis of the regularization term. 
When $\kappa$ is decreasing (for example, $\kappa(t) = e^{-t}$ or $(1+t)^{-p}$), the kernel assigns larger weights to nearby pairs, thus promoting preservation of local geometric structures. 
Conversely, when $\kappa$ is increasing but bounded (for example, $\kappa(t)=\exp(t)/(1+\exp(t))$), it emphasizes distant pairs, discouraging global contraction and encouraging large-scale geometric alignment. 
In both cases, the bandwidth $h$ controls the spatial scale of this emphasis—small $h$ focuses on fine local structures, while large $h$ smooths over broader spatial relationships. 
Finally, the boundedness of $\kappa$ ensures that $K_h$ is bounded and that the operator $D_X^{\kappa}$ is Hilbert--Schmidt; importantly, the convexity result in Theorem~\ref{thm:convex-relaxation} holds regardless of the monotonicity direction, which will be discussed later.

\begin{definition}[Distance potential operator]
	\label{def:distance-potential}
	Let $(S,d_S,\mathbb{P}_X)$ be a compact metric space. The \emph{distance potential operator} $D_X^{\kappa} : L^2(S,\mathbb{P}_X) \to L^2(S,\mathbb{P}_X)$ is defined by
	\begin{align*}
		(D_X^{\kappa}f)(x) \coloneqq \mathbb{E}\left[K_h(x,X)f(X)\right] = \int_S K_h(x,y)\, f(y)\, \mathbb{P}_X(dy) , \quad \forall f \in L^2(S,\mathbb{P}_X),\ \forall x \in S .
	\end{align*}
\end{definition}
The distance potential operator is a special case of a Hilbert-Schmidt operator. Intuitively, $(D_X^{\kappa} f)(x)$ represents a distance-weighted average of $f$ with respect to the point $x$.

\begin{lemma}
	\label{lem:regularization-term}
	For any $\pi \in \Pi(\mathbb{P}_X,\mathbb{P}_Y)$,
	\begin{align*}
		\| D_X^{\kappa}T_{\pi} - T_{\pi}D_Y^{\kappa} \|_{\mathrm{HS}}^2 = \int_S\int_S \Gamma_\pi(x,y)^2\mathbb{P}_X(dx)\mathbb{P}_Y(dy) ,
	\end{align*}
	where $\Gamma_\pi^1(x,y) \coloneqq \mathbb{E}_\pi[K_h(x,X) \mid Y = y]$, $\Gamma_\pi^2(x,y) \coloneqq \mathbb{E}_\pi[K_h(y,Y) \mid X = x]$, and $\Gamma_\pi(x,y) \coloneqq \Gamma_\pi^1(x,y) - \Gamma_\pi^2(x,y)$.
\end{lemma}
\begin{proof}
	See Appendix~\ref{pf:lem:regularization-term}.
\end{proof}
\cref{lem:regularization-term} proposes a convex surrogate for the Gromov-Wasserstein term in \eqref{eq:pi-fused-ot}, which can be explicitly expressed as the squared integral of a kernel function $\Gamma_\pi(x,y)$. This kernel $\Gamma_\pi$ measures the discrepancy between two conditional expected distances: $\Gamma_\pi^1(x,y)$ and $\Gamma_\pi^2(x,y)$. Specifically, $\Gamma_\pi^1(x,y)$ represents the expected kernel value $K_h(x,X)$ given $Y=y$, while $\Gamma_\pi^2(x,y)$ is the expected kernel value $K_h(y,Y)$ given $X=x$. The regularization term can thus be interpreted as a metric that quantifies the symmetric alignment of these ``cross-spatial" kernel-weighted expectations induced by the coupling $\pi$.

\begin{theorem}
	\label{thm:convex-relaxation}
	For $0 \leq \alpha \leq 1$ and a feature function $f$, consider the optimization problem 
	\begin{align}
		\label{eq:proposed-method}
		\inf_{\pi\in\Pi(\mathbb{P}_X,\mathbb{P}_Y)} (1-\alpha)\mathbb{E}_{(X,Y)\sim\pi}\big[\|f(X)-f(Y)\|_2^2\big] + \frac{\alpha}{2} \| D_X^{\kappa}T_\pi - T_\pi D_Y^{\kappa} \|_{\mathrm{HS}}^2 ,
	\end{align}
	where $\|\cdot\|_{\mathrm{HS}}^2$ is a Hilbert-Schmidt norm. Then, \eqref{eq:proposed-method} is a convex problem.
\end{theorem}
\begin{proof}
	See Appendix~\ref{pf:thm:convex-relaxation}.
\end{proof}
\cref{thm:convex-relaxation} establishes that the proposed optimization problem \eqref{eq:proposed-method} is convex with respect to the coupling $\pi$. This stems from the fact that the first term (feature-wise alignment) is linear in $\pi$, and the second regularization term is also a convex function of $\pi$. The latter holds because the map $\pi \mapsto T_\pi$ is affine, and the squared Hilbert-Schmidt norm $\|\cdot\|_{\mathrm{HS}}^2$ is a convex function; their composition thus preserves convexity (as detailed in Appendix~\ref{pf:thm:convex-relaxation}). Consequently, this problem formulation circumvents the computational challenges arising from non-convexity, which are inherent to the original Fused Gromov-Wasserstein problem~\eqref{eq:pi-fused-ot}, and guarantees that a global optimum can be efficiently found.

%%% Illustration %%%
%\begin{proposition}
%	\label{prop:solution}
%	Let $T: S \to S$ be an isometry and $\pi = (\mathrm{Id},T)_{\#}\mathbb{P}_X$. Then, $\|D_X^{\kappa}T_{\pi} - T_{\pi}D_Y^{\kappa}\|_{\mathrm{HS}}^2 = 0$.
%\end{proposition}
%\begin{proof}
%	First, note that $Y \overset{a.s.}{=} T(X)$. Our goal is to show that $\Gamma_\pi(x,y) = 0$ for $\pi$-a.e. $(x,y) \in S^2$.
%	\begin{align*}
%		\Gamma_\pi^1(x,y) \overset{a.s.}{=} d_S(x,T^{-1}(y)) = d_S(T(x),y) \overset{a.s.}{=} \mathbb{E}\left[d_S(Y,y) \mid X = x\right] = \Gamma_\pi^2(x,y) .
%	\end{align*}
%\end{proof}
\begin{proposition}
	\label{prop:solution}
	Let $T: S \to S$ be an injective measurable map, and consider $\pi = (\mathrm{Id},T)_{\#}\mathbb{P}_X$. Then,
	\begin{align*}
		\|D_X^{\kappa}T_{\pi} - T_{\pi}D_Y^{\kappa}\|_{\mathrm{HS}}^2 = 0 \;\Longleftarrow\;d_S(T(x),T(x^\prime)) = d_S(x,x^\prime) ,\;\; \text{for $\mathbb{P}_X \otimes \mathbb{P}_X$-a.e. $(x,x^\prime)$} .
	\end{align*}
	The converse holds if $\kappa$ is further assumed to be strictly monotone. Moreover, if in addition $T$ is continuous and $\mathrm{supp}(\mathbb{P}_X) = S$, then the identity $d_S(T(x),T(x'))=d_S(x,x')$ holds for all $x,x'\in S$, hence $T$ is an isometry on $S$.
\end{proposition}
\begin{proof}
	See Appendix~\ref{pf:prop:solution}.
\end{proof}
\cref{prop:solution} provides a crucial validation for the proposed regularization term, demonstrating its consistency with the goals of geometric structure preservation. It shows that the regularization term vanishes if and only if the coupling $\pi$ is induced by a deterministic almost-isometry $T$. This equivalence ($\iff$) is a powerful consequence of the strictly monotone property of the kernel function $\kappa$ (\cref{def:kernel}), which ensures that the kernel values are identical ($K_h(x,x') = K_h(T(x),T(x'))$) if and only if the underlying distances are identical ($d_S(x,x') = d_S(T(x),T(x'))$). This confirms that our convex surrogate correctly and exclusively identifies these ideal, structure-preserving maps as optimal solutions for the structural part of the problem, mimicking the behavior of the original Gromov-Wasserstein discrepancy.

%%% Non-uniqueness %%%
Indeed, this also reveals that the objective \eqref{eq:proposed-method} is generally not strictly convex; for instance, if multiple distinct isometries exist and they produce the same feature-matching cost, they will all be global minimizers.

\paragraph{Motivation.}  
We provide a motivation for comparing our convex surrogate with the Gromov--Wasserstein penalty.  
Setting $\kappa(x) = x$, consider the following $3 \times 3$ matching problem:
\begin{align*}
	D_X = \begin{pmatrix}
		0 & x_{12} & x_{13} \\
		x_{12} & 0 & x_{23} \\
		x_{13} & x_{23} & 0
	\end{pmatrix} , 
	\quad
	D_Y = \begin{pmatrix}
		0 & y_{12} & y_{13} \\
		y_{12} & 0 & y_{23} \\
		y_{13} & y_{23} & 0
	\end{pmatrix} .
\end{align*}
For clarity, we consider the trajectory $3\pi(t) = tI + (1-t)P$ over $0 \leq t \leq 1$,  
where $P$ is the permutation matrix obtained by switching the second and third rows of the identity matrix, that is,
\begin{align*}
	I = \begin{pmatrix}
		1 & 0 & 0 \\
		0 & 1 & 0 \\
		0 & 0 & 1
	\end{pmatrix} , \quad
	P = \begin{pmatrix}
		1 & 0 & 0 \\
		0 & 0 & 1 \\
		0 & 1 & 0
	\end{pmatrix} , \quad
	\pi(t) = \frac{1}{3} \left(tI + (1-t)P\right) , \quad 0 \leq t \leq 1 . 
\end{align*} 
A straightforward calculation yields
\begin{align*}
	\|D_XT_{\pi(t)} - T_{\pi(t)}D_Y \|_{\mathrm{HS}}^2 = \| D_X\pi(t) - \pi(t)D_Y \|_F^2 = \frac{1}{9}(at^2 + bt + c) ,
\end{align*}
where
\begin{align*}
	&A = 2\left[(x_{12} - y_{12})^2 + (x_{13} - y_{13})^2 + (x_{23} - y_{23})^2\right] , \\
	&B = 2\left[(x_{13} - y_{12})^2 + (x_{12} - y_{13})^2 + (x_{23} - y_{23})^2\right] , \\
	&C = (x_{12} + x_{13} - y_{12} - y_{13})^2 , \\[5pt]
	&a = A + B - 2C , \quad b = -2B + 2C , \quad c = B .
\end{align*}
Thus, when $a \neq 0$, the unique minimizer $t_{\mathrm{CS}}^\ast$ is given by 
\begin{align*}
	t_{\mathrm{CS}}^\ast = \frac{B - C}{(A - C) + (B - C)} .
\end{align*}
Similarly, we can readily derive
\begin{align*}
	\mathbb{E}_{\substack{(X,Y)\sim\pi\\(X^\prime,Y^\prime)\sim\pi}}
	\Big[\big|d_S(X,X^\prime)-d_S(Y,Y^\prime)\big|^2\Big] = \mathrm{const.} - \frac{2}{3}\mathrm{Tr}(D_X \pi(t) D_Y \pi(t)^\top) ,
\end{align*}
and
\begin{align*}
	&\mathrm{Tr}(D_X \pi(t) D_Y \pi(t)^\top) = D t^2 + E (1 - t)^2 + Ft(1-t) , \\[5pt]
	&D = 2(x_{12}y_{12} + x_{13}y_{13} + x_{23}y_{23}) , \\
	&E = 2(x_{13}y_{12} + x_{12}y_{13} + x_{23}y_{23}) , \\
	&F = 2(x_{12}+x_{13})(y_{12} + y_{13}) .
\end{align*}
Since the second derivative of the above expression is positive (equal to $8x_{23}y_{23} > 0$),  
the GW penalty is concave in $t$.  
Comparing the two endpoints, the minimizer $t_{\mathrm{GW}}^\ast$ can be expressed as
\begin{align*}
	t_{\mathrm{GW}}^\ast = \begin{cases}
		%		0 & (x_{12} - y_{12})^2 + (x_{13} - y_{13})^2 \geq (x_{12} - y_{13})^2 + (x_{13} - y_{12})^2 , \\
		%		1 & (x_{12} - y_{12})^2 + (x_{13} - y_{13})^2 \leq (x_{12} - y_{13})^2 + (x_{13} - y_{12})^2 . 
		0 & A \geq B , \\
		1 & A \leq B .
	\end{cases}
\end{align*}

%%% When A >> B %%%
\begin{figure}[t]
	\centering
%	\begin{subfigure}[t]{0.33\textwidth}
%		\centering
%		\begin{tikzpicture}[scale=0.8, baseline=(current bounding box.north)]
%				\path[use as bounding box] (1.5,3.5) rectangle (10.8,9.2);
%				
%				% X set
%				\coordinate (X1) at (4,8);
%				\coordinate (X2) at (2,4);
%				\coordinate (X3) at (5,6);
%				
%				% Y set
%				\coordinate (Y1) at (8,8);
%				\coordinate (Y2) at (9,6 - 0.7);
%				\coordinate (Y3) at (6,4);
%				
%				% Edges 
%				\foreach \i/\j in {1/2,2/3,1/3}{
%						\draw[black, thin] (X\i)--(X\j);
%						\draw[black, thin] (Y\i)--(Y\j); 
%					}
%				
%				% Points + Labels
%				\fill (X1) circle(1.5pt) node[above right] {$X_{1}$}; 
%				\fill (Y1) circle(1.5pt) node[above right] {$Y_{1}$}; 
%				
%				\fill (X2) circle(1.5pt) node[below left] {$X_{2}$}; 
%				\fill (Y2) circle(1.5pt) node[above right] {$Y_{2}$}; 
%				
%				\fill (X3) circle(1.5pt) node[above right] {$X_{3}$}; 
%				\fill (Y3) circle(1.5pt) node[below left] {$Y_{3}$}; 
%				
%				% Edge labels
%				\node at ($(X1)!0.5!(X2)+(-0.1,0.6)$) {$x_{12}$};
%				\node at ($(X2)!0.5!(X3)+(0.7,-0.1)$) {$x_{23}$};
%				\node at ($(X1)!0.5!(X3)+(0.4,0.1)$) {$x_{13}$};
%				
%				\node at ($(Y1)!0.5!(Y2)+(0.4,0.1)$) {$y_{12}$}; 
%				\node at ($(Y2)!0.5!(Y3)+(0.7,-0.1)$) {$y_{23}$};
%				\node at ($(Y1)!0.5!(Y3)+(-0.1,0.6)$) {$y_{13}$};
%			\end{tikzpicture}
%		\caption{Geometry}
%	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{../simulation/motivating_geometry.png}
		\caption{Geometric structure of source and target samples}
		\label{fig:motivating-geometry}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../simulation/convex_vs_gw.png}
		\caption{Proposed convex surrogate and GW penalty}
		\label{fig:motivating-penalty-trajectory}
	\end{subfigure}
	\caption{Comparison of the geometric structures and associated penalty functions}
	\label{fig:motivating-example}
\end{figure}
%%% Figure explanation: A >> B %%%
Figure~\ref{fig:motivating-example} visualizes the case when $A \gg B \approx 0$. While both penalties encourage small values of $t$, the proposed convex surrogate attains its minimum near zero, whereas the GW penalty reaches its minimum exactly at $t = 0$. Nevertheless, due to its concave shape, the GW objective may converge to the opposite extreme ($t = 1$) depending on the initialization.

%%% Conclusion %%%
Overall, this example highlights the difference between the proposed convex surrogate and the GW penalty. While the GW objective exhibits a concave behavior that often results in extreme solutions (corresponding to permutation-like transport plans), our convex surrogate yields a smooth and well-behaved solution. In particular, the minimizer $t_{\mathrm{CS}}^\ast$ varies continuously with the geometric discrepancy, while ensuring the convexity. This property ensures numerical stability and uniqueness of the solution, making the convex surrogate more suitable for optimization and statistical analysis, especially in complex settings where the GW 
penalty may fall into suboptimal solutions due to its non-convexity.

\section{Algorithm}
Suppose that we have $(X_i,f(X_i))$ for $i=1,...,n_X$ as source data and $(Y_j,f(Y_j))$ for $j=1,...,n_Y$ as target data. Denote $\hat{\mathbb{P}}_X$ and $\hat{\mathbb{P}}_Y$ by the empirical distributions of $X$ and $Y$, respectively. Then, the empirical version of \eqref{eq:proposed-method} corresponds to
\begin{align*}
	%\label{eq:finite-proposed-method}
	\inf_{\pi \in \Pi(\hat{\mathbb{P}}_X,\hat{\mathbb{P}}_Y)} (1-\alpha)\mathbb{E}_{(X,Y)\sim\pi}\big[\|f(X)-f(Y)\|_2^2\big] + \frac{\alpha}{2} \| D_X^{\kappa}T_\pi - T_\pi D_Y^{\kappa} \|_{\mathrm{HS}}^2 .
\end{align*}
In fact, the above problem can be written as a convex quadratic program (and reduces to a linear program when $\alpha = 0$):
\begin{align}
	\label{eq:finite-proposed-method}
	\min_{\pi}\; &\underbrace{(1-\alpha)\mathrm{Tr}\left(C_f^\top \pi\right) + \frac{\alpha}{2n_Xn_Y} \left\| n_Y \hat{D}^{\kappa}_X \pi - n_X \pi \hat{D}^{\kappa}_Y \right\|_F^2}_{\eqqcolon \mathcal{L}(\pi)} \nonumber\\
	\mathrm{s.t.}&\quad \pi\mathbf{1}_{n_Y} = \frac{1}{n_X} \mathbf{1}_{n_X} , \quad \pi^\top \mathbf{1}_{n_X} = \frac{1}{n_Y} \mathbf{1}_{n_Y} , \quad \pi \geq 0 ,
\end{align}
where $\pi \in \mathbb{R}_{+}^{n_X \times n_Y}$, $(C_f)_{ij} = \|f(X_i) - f(Y_j)\|_2^2$, $(\hat{D}^{\kappa}_X)_{ii^\prime} = K_h(X_i,X_{i^\prime})$, and $(\hat{D}^{\kappa}_Y)_{jj^\prime} = K_h(Y_j,Y_{j^\prime})$.

%%% Optimization algorithm %%%
As \eqref{eq:finite-proposed-method} is a convex quadratic program, numerous standard optimization algorithms are available to find its global minimizer. In this paper, however, we focus on the Frank-Wolfe (FW) algorithm, also known as the conditional gradient (CG) method. The FW algorithm is particularly well-suited for this problem due to its ``projection-free" nature. Unlike projected gradient methods that require a potentially costly projection back onto the feasible set $\Pi(\hat{\mathbb{P}}_X,\hat{\mathbb{P}}_Y)$ at each iteration, the FW algorithm only requires solving a linear minimization problem over this same set. This linear subproblem (or "Linear Minimization Oracle") is often computationally simpler and more efficient to solve than the full projection, making the FW algorithm an attractive choice for optimization problems over the transport polytope.

%%% Find an optimal transport %%%
In many practical applications, it is often more desirable to find a deterministic transport map (or hard assignment) from $X$ to $Y$, rather than the soft coupling $\hat{\pi}$ found by \eqref{eq:finite-proposed-method}. While the optimal solution $\hat{\pi}$ is not guaranteed to be deterministic, we can obtain such a map by solving the following linear assignment problem (LAP):
\begin{align}
	\label{eq:finite-projection-method}
	\hat{P} \coloneqq \argmax_{P \in \mathcal{P}} \mathrm{Tr}(P^\top\hat{\pi}) ,
\end{align}
where $\hat{\pi}$ is an optimal solution to \eqref{eq:finite-proposed-method} and $\mathcal{P}$ denotes the set of deterministic assignment matrices. Specifically, 
\begin{align}
	\label{eq:assign-matrix-space}
	\mathcal{P} \coloneqq \{P \in \{0,1\}^{n_X \times n_Y}: P\mathbf{1}_{n_Y} \leq \mathbf{1}_{n_X}, \; P^\top\mathbf{1}_{n_X} \leq \mathbf{1}_{n_Y}\} .
\end{align}
This LAP seeks the hard assignment $P$ that best aligns with the optimal soft coupling $\hat{\pi}$ and can be solved efficiently using standard methods like the Hungarian algorithm.

\begin{algorithm}[t]
	\caption{Convex Quadratic Fused Transport Plan via FW and LAP Projection}
	\label{alg:proposed-algorithm}
	\begin{algorithmic}[1]
		\Require Source data $\{(X_i, f(X_i))\}_{i=1}^{n_X}$, target data $\{(Y_j, f(Y_j))\}_{j=1}^{n_Y}$, weight parameter $0 \leq \alpha \leq 1$, distance kernel $K_h$, max iters $T$
		
		\State Construct matrices:
		\Statex \hspace{\algorithmicindent} $(C_f)_{ij} \gets \|f(X_i)-f(Y_j)\|_2^2$, \quad
		$(\hat D_X^{\kappa})_{ii'} \gets K_h(X_i,X_{i'})$, \quad
		$(\hat D_Y^{\kappa})_{jj'} \gets K_h(Y_j,Y_{j'})$
		
		%\Statex \hspace{\algorithmicindent} $A \gets n_Y \hat D_X^{\kappa}$, \quad $B \gets n_X \hat D_Y^{\kappa}$
		%\State Initialize $\pi^{(0)} \gets \frac{1}{n_X n_Y}\mathbf{1}_{n_X}\mathbf{1}_{n_Y}^\top$
		\State Initialize $\pi^{(0)} \gets \hat{\mathbb{P}}_X \otimes \hat{\mathbb{P}}_Y$
		
		\For{$t=0,...,T$}
		%\State $\nabla F(\pi^{(t)}) \gets C_f + \frac{\lambda}{n_X n_Y}\!\left[A^\top(A\pi^{(t)}-\pi^{(t)}B) - (A\pi^{(t)}-\pi^{(t)}B)B^\top\right]$
		\State Calculate the gradient $\nabla \mathcal{L}(\pi^{(t)})$ in \eqref{eq:finite-proposed-method}
		\State Take $\tilde{\pi}^{(t)} \gets \arg\min_{\pi \in \Pi(\hat{\mathbb{P}}_X,\hat{\mathbb{P}}_Y)} \mathrm{Tr}(\nabla \mathcal{L}(\pi^{(t)})^\top \pi)$
		\State $\pi^{(t+1)} \gets (1-\gamma_t)\pi^{(t)} + \gamma_t \tilde{\pi}^{(t)}$ for some $0 < \gamma_t < 1$
		\EndFor
		\State $\hat\pi \gets \pi^{(T)}$
		\State \textbf{Optional (LAP projection):} $\displaystyle
		\hat P \gets \arg\max_{P \in \mathcal{P}} \mathrm{Tr}(P^\top \hat\pi)$, where $\mathcal{P}$ is as defined in \eqref{eq:assign-matrix-space}
		\State \textbf{Return:} $\hat\pi$ (and optionally $\hat P$)
	\end{algorithmic}
\end{algorithm}




%%% References %%%
\bibliographystyle{abbrvnat}
\bibliography{FOT_references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%          %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Appendix %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%          %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\section{Appendix}
\subsection{Proof for \cref{lem:regularization-term}}\label{pf:lem:regularization-term}
\begin{proof}
	For $g \in L^2(S,\mathbb{P}_Y)$, observe that
	\begin{align*}
		&(D_X^{\kappa}T_{\pi}g)(x) = \mathbb{E}\Big[K_h(x,X)\mathbb{E}\left[g(Y) \mid X\right]\Big] = \int_S \Gamma_\pi^1(x,y)g(y)\mathbb{P}_Y(dy) , \\
		&(T_{\pi}D_Y^{\kappa}g)(x) = \mathbb{E}\Big[(D_Y^{\kappa}g)(Y) \mid X = x\Big] = \int_S \Gamma_\pi^2(x,y)g(y) \mathbb{P}_Y(dy) .
	\end{align*}
	Since $K_h$ is bounded, $D_X^{\kappa}T_{\pi} - T_{\pi}D_Y^{\kappa}$ is a well-defined Hilbert-Schmidt operator with a kernel $\Gamma_\pi(x,y)$:
	\begin{align*}
		\| D_X^{\kappa}T_{\pi} - T_{\pi}D_Y^{\kappa} \|_{\mathrm{HS}}^2 = \int_S\int_S \Gamma_\pi(x,y)^2\mathbb{P}_X(dx)\mathbb{P}_Y(dy) \leq \frac{(\sup\kappa)^2}{h^2} .
	\end{align*}
\end{proof}

\subsection{Proof for \cref{thm:convex-relaxation}}\label{pf:thm:convex-relaxation}
\begin{proof}
	Refer to \cref{lem:regularization-term} to confirm that the operator $D_X^{\kappa}T_{\pi} - T_{\pi}D_Y^{\kappa}$ is well-defined and Hilbert--Schmidt for any $\pi \in \Pi(\mathbb{P}_X,\mathbb{P}_Y)$. 
	To establish convexity of \eqref{eq:proposed-method}, it suffices to show that 
	\begin{align*}
	\pi \;\mapsto\; \|D_X^{\kappa}T_\pi - T_\pi D_Y^{\kappa}\|_{\mathrm{HS}}^2
	\end{align*}
	is convex on $\Pi(\mathbb{P}_X,\mathbb{P}_Y)$.
	
	Let $\pi_1,\pi_2 \in \Pi(\mathbb{P}_X,\mathbb{P}_Y)$ and $t \in [0,1]$, and define $\pi_t = t\pi_1 + (1-t)\pi_2$. 
	Since both $\pi_1$ and $\pi_2$ share the same marginals $\mathbb{P}_X$ and $\mathbb{P}_Y$, 
	the disintegration theorem ensures that their corresponding conditional kernels satisfy
	\begin{align*}
	k_t(\cdot|x) = t\,k_1(\cdot|x) + (1-t)\,k_2(\cdot|x)
	\quad \text{for} \;\; \mathbb{P}_X\text{-a.e.} \; x \in S.
	\end{align*}
	That is, the conditional distribution of $Y$ given $X=x$ under $\pi_t$ is the convex combination of the conditional distributions under $\pi_1$ and $\pi_2$. 
	Consequently, for any $g \in L^2(S,\mathbb{P}_Y)$,
	\begin{align*}
		(T_{\pi_t} g)(x)
		&= \int g(y)\,k_t(dy|x)
		= t \int g(y)\,k_1(dy|x) + (1-t)\int g(y)\,k_2(dy|x) \\
		&= t\,(T_{\pi_1}g)(x) + (1-t)\,(T_{\pi_2}g)(x),
	\end{align*}
	which shows that $T_{\pi_t}$ depends affinely on $\pi$, i.e.,
	\begin{align*}
	T_{\pi_t} = t\,T_{\pi_1} + (1-t)\,T_{\pi_2}.
	\end{align*}
	
	Because $D_X^{\kappa}$ and $D_Y^{\kappa}$ are linear operators, it follows that
	\begin{align*}
	D_X^{\kappa}T_{\pi_t} - T_{\pi_t}D_Y^{\kappa}
	= t\,(D_X^{\kappa}T_{\pi_1} - T_{\pi_1}D_Y^{\kappa})
	+ (1-t)\,(D_X^{\kappa}T_{\pi_2} - T_{\pi_2}D_Y^{\kappa}).
	\end{align*}
	Denoting $A_i := D_X^{\kappa}T_{\pi_i} - T_{\pi_i}D_Y^{\kappa}$ $(i=1,2)$ and $A_t := D_X^{\kappa}T_{\pi_t} - T_{\pi_t}D_Y^{\kappa}$, 
	we have $A_t = tA_1 + (1-t)A_2$. 
	Since $\|\cdot\|_{\mathrm{HS}}^2$ is convex, we obtain
	\begin{align*}
	\|A_t\|_{\mathrm{HS}}^2rm
	= \|tA_1 + (1-t)A_2\|_{\mathrm{HS}}^2
	\le t\,\|A_1\|_{\mathrm{HS}}^2 + (1-t)\,\|A_2\|_{\mathrm{HS}}^2.
	\end{align*}
	Therefore, $\pi \mapsto \|D_X^{\kappa}T_\pi - T_\pi D_Y^{\kappa}\|_{\mathrm{HS}}^2$ is convex on $\Pi(\mathbb{P}_X,\mathbb{P}_Y)$. 
\end{proof}

\subsection{Proof for \cref{prop:solution}}\label{pf:prop:solution}
\begin{proof}
	Since $Y = T(X)$ almost surely, for any $(x,y) \in \mathrm{supp}(\pi)$, we have
	\begin{align*}
		\Gamma_\pi^1(x,y) &= \mathbb{E}[K_h(x,X)\mid Y=y] = \frac{1}{h}\kappa\left(\frac{d_S\bigl(x,T^{-1}(y)\bigr)}{h}\right), \\
		\Gamma_\pi^2(x,y) &= \mathbb{E}[K_h(y,Y)\mid X=x] = \frac{1}{h}\kappa\left(\frac{d_S\bigl(y,T(x)\bigr)}{h}\right).
	\end{align*}
	Hence, for such $(x,y)$,
	\begin{align*}
		\Gamma_\pi(x,y)
		= \Gamma_\pi^1(x,y) - \Gamma_\pi^2(x,y)
		= \frac{1}{h}\left[\kappa\left(\frac{d_S\bigl(x,T^{-1}(y)\bigr)}{h}\right) - \kappa\left(\frac{d_S\bigl(y,T(x)\bigr)}{h}\right)\right] .
	\end{align*}
	Now, setting $y = T(x')$ gives
	\begin{align*}
		\Gamma_\pi\bigl(x,T(x')\bigr)
		= \frac{1}{h}\left[\kappa\left(\frac{d_S\bigl(x,x^\prime\bigr)}{h}\right) - \kappa\left(\frac{d_S\bigl(T(x),T(x^\prime)\bigr)}{h}\right)\right] .
	\end{align*}
	
	($\Leftarrow$) If $d_S(T(x),T(x')) = d_S(x,x')$ holds for $\mathbb{P}_X \otimes \mathbb{P}_X$-almost every $(x,x')$, substituting this into the above expression yields $\Gamma_\pi(x,T(x')) = 0$ for $\mathbb{P}_X \otimes \mathbb{P}_X$-almost every $(x,x')$. Consequently, $\Gamma_\pi = 0$ holds for $\pi$-almost every $(x,y)$, which implies that the Hilbert–Schmidt norm is zero.
	
	($\Rightarrow$) Now, assume that $\kappa$ is strictly monotone. If $\|D_X^{\kappa}T_\pi - T_\pi D_Y^{\kappa}\|_{\mathrm{HS}}^2 = 0$, then $\Gamma_\pi = 0$ holds for $\pi$-almost every $(x,y)$, and in particular, for pairs $(x,T(x'))$ with respect to $\mathbb{P}_X \otimes \mathbb{P}_X$-almost every $(x,x')$. By the strict monotone property of $\kappa$, $d_S(T(x),T(x^\prime)) = d_S(x,x^\prime)$ holds for $\mathbb{P}_X \otimes \mathbb{P}_X$-almost every $(x,x^\prime)$.
	
	Finally, if $T$ is continuous and $\mathrm{supp}(\mathbb{P}_X) = S$, then the function
	\begin{align*}
		F(x,x') := K_h(x,x') - K_h(T(x),T(x'))
	\end{align*}
	is continuous and vanishes on the dense set $\mathrm{supp}(\mathbb{P}_X) \times \mathrm{supp}(\mathbb{P}_X)$.  
	By continuity, $F \equiv 0$ on $S \times S$.
	Again by the strict monotonicity of $\kappa$, $d_S(T(x),T(x')) = d_S(x,x')$ holds for all $x,x' \in S$, implying that $T$ is an isometry on $S$.
\end{proof}


\end{document}